# Training Improvements - Rwanda Farmer Chatbot ğŸš€

## Problem Identified

Your uploaded model was giving poor responses (just echoing keywords) because it was the **base t5-small model** without fine-tuning on your agriculture dataset.

Example poor responses:
```
Q: How can I prevent maize stem borer?
A: prevent borer  âŒ

Q: What fertilizer should I use for tomatoes?
A: fertilizer  âŒ
```

## Root Cause

The model weights in `models/baseline_final` were the pre-trained t5-small weights, not your fine-tuned weights trained on the agriculture QA dataset.

## Solutions Implemented

### 1. **Optimized Training Hyperparameters** âš™ï¸

**Before (baseline settings):**
- Epochs: 3
- Learning Rate: 5e-5
- Gradient Accumulation: 2 (effective batch = 16)
- Warmup Steps: 100

**After (optimized settings):**
- **Epochs: 6** (doubled for better learning)
- **Learning Rate: 3e-5** (lower for stable fine-tuning)
- **Gradient Accumulation: 4** (effective batch = 32 for better stability)
- **Warmup Steps: 200** (more gradual learning)

### Why These Changes Matter:

| Setting | Impact |
|---------|--------|
| **More Epochs (6)** | Model sees data more times â†’ better learning â†’ better answers |
| **Lower LR (3e-5)** | Smaller weight updates â†’ more stable training â†’ avoids overfitting |
| **Larger Effective Batch (32)** | More stable gradients â†’ smoother training â†’ better convergence |
| **More Warmup (200)** | Gradual learning rate increase â†’ prevents early instability |

### 2. **Added Complete Training Cell** ğŸ”§

The notebook was missing the actual training code! I added a complete training cell that:

âœ… Initializes the Trainer with your dataset  
âœ… Runs training for 6 epochs  
âœ… Saves the **fine-tuned** model to `models/baseline_final`  
âœ… Saves training plots to visualize learning progress  
âœ… Works in both local environment and Google Colab  

### 3. **Fixed Model Save Path** ğŸ’¾

Ensured the model saves to the correct location:
- Local: `./models/baseline_final`
- Colab: `/content/drive/MyDrive/agribot/models/baseline_final`

The saved model now includes:
- âœ… `pytorch_model.bin` or `model.safetensors` (actual weights)
- âœ… `config.json`
- âœ… `tokenizer_config.json`
- âœ… `special_tokens_map.json`
- âœ… `spiece.model`

## Expected Results ğŸ“Š

### Training Time
- **On GPU (Colab)**: ~20-30 minutes for 6 epochs
- **On CPU (Local)**: ~2-3 hours for 6 epochs

### Expected Performance
After training with optimized settings, you should see:
- **BLEU Score**: 0.30-0.45
- **ROUGE-L**: 0.35-0.50
- **F1 Score**: 0.40-0.55

### Example Good Responses (After Training)

```
Q: How can I prevent maize stem borer?
A: Use resistant varieties, practice crop rotation, remove and destroy 
   infested plants, apply appropriate insecticides during early infestation. âœ…

Q: What fertilizer should I use for tomatoes?
A: Use NPK fertilizer with a ratio of 10-10-10 or 15-15-15. Apply 
   organic compost before planting and side-dress with nitrogen during 
   the growing season. âœ…
```

## How to Run Training

### Option 1: Google Colab (Recommended - FREE GPU)

1. Open notebook in Colab: Click the "Open in Colab" badge
2. Run the **Colab Setup** cells (Steps 1-4)
3. Run all cells in order
4. Wait ~20-30 minutes for training
5. Model saves to Google Drive automatically

### Option 2: Local Environment

1. Activate your virtual environment:
   ```bash
   source venv/bin/activate
   ```

2. Run the notebook cells in order

3. Training will take longer on CPU (~2-3 hours)

## After Training - Upload to Hugging Face

Once training completes successfully:

```bash
# The model is already saved to models/baseline_final
# Just run the upload script
python upload_model_to_hf.py
```

This will upload your **fine-tuned** model (not the base model) to:
- https://huggingface.co/ngabodevv/rwanda-farmer-chatbot-t5

## Verification Steps

After training, test the model:

```python
from transformers import T5Tokenizer, T5ForConditionalGeneration

tokenizer = T5Tokenizer.from_pretrained('models/baseline_final')
model = T5ForConditionalGeneration.from_pretrained('models/baseline_final')

question = "How can I prevent maize stem borer?"
inputs = tokenizer(f"question: {question}", return_tensors='pt')
outputs = model.generate(**inputs, max_length=256, num_beams=4)
answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(f"Q: {question}")
print(f"A: {answer}")
```

You should now get meaningful, agriculture-specific answers!

## Summary

| Aspect | Before | After |
|--------|--------|-------|
| **Model** | Base t5-small | Fine-tuned on agriculture data |
| **Epochs** | 3 | 6 |
| **Learning Rate** | 5e-5 | 3e-5 |
| **Effective Batch** | 16 | 32 |
| **Answer Quality** | Keywords only | Full, helpful answers |
| **Training** | Missing code | Complete training pipeline |
| **Save Path** | Incorrect | Fixed for local & Colab |

## Next Steps

1. âœ… Run the updated notebook cells
2. âœ… Wait for training to complete (monitor loss decreasing)
3. âœ… Verify model gives good answers
4. âœ… Upload to Hugging Face
5. âœ… Test in Gradio app (`python app.py`)
6. âœ… Create demo video for your submission

---

**Happy Training! ğŸŒ¾ğŸ‡·ğŸ‡¼**

If you encounter any issues during training, check:
- GPU is enabled in Colab (Runtime â†’ Change runtime type â†’ GPU)
- All setup cells ran successfully
- Dataset loaded correctly (should have 22,615 samples)
